{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EncoderDecoderLSTM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOg9TPiw7essT/OlWmA4Gx8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ASclAIPSgJa"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "_98D0qc9Sk4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "aWyxZ4W_Sl81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_folder = '/content/drive/My Drive/WordGeneration'"
      ],
      "metadata": {
        "id": "e-GYyNJJSoau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(root_folder+'/FOMC2021.txt', sep=\"\\n\")"
      ],
      "metadata": {
        "id": "vSXiAxdWSovG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.rename(columns={\"Action to Adopt Changes to the Committee's Rules Regarding Availability of Information\": \"text\"},\n",
        "          inplace=True)\n",
        "data[\"text\"] = data[\"text\"].str.replace(\"United States\", \"US\")\n",
        "data[\"text\"] = data[\"text\"].str.replace(\"U.S.\", \"US\")\n",
        "data.info()"
      ],
      "metadata": {
        "id": "3LNv11ExSpoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "func_folder = '/content/drive/My Drive/Colab Notebooks'"
      ],
      "metadata": {
        "id": "WXmagxkpSq10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(func_folder)"
      ],
      "metadata": {
        "id": "S149SfIqStIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import Contractions\n",
        "from Contractions import *"
      ],
      "metadata": {
        "id": "Mbt_uvYQSuGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(text):\n",
        "    text = re.sub('[0-9]+.\\t', '', str(text)) # removing paragraph numbers\n",
        "    text = re.sub('U.S.', 'USA', str(text))\n",
        "    text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split(' ')])\n",
        "    text = re.sub('\\n ', '', str(text))\n",
        "    text = re.sub('\\n', ' ', str(text))\n",
        "    text = re.sub(\"'s\", '', str(text))\n",
        "    text = re.sub(\"-\", ' ', str(text))\n",
        "    text = re.sub(\"â€” \", '', str(text))\n",
        "    text = re.sub('\\\"', '', str(text))\n",
        "    text = re.sub(\"Mr\\.\", 'Mr', str(text))\n",
        "    text = re.sub(\"Mrs\\.\", 'Mrs', str(text))\n",
        "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", str(text))\n",
        "    text = re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', text) # add space around punctuation, i.e. treat them as token\n",
        "    text = re.sub(r'\\s\\s', ' ', text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "CAZ16OlnSvBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['text_clean'] = data['text'].apply(clean)"
      ],
      "metadata": {
        "id": "OVeN1yxQSwPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sequence_generator(texts,\n",
        "                      training_length, \n",
        "                      result_length, \n",
        "                      max_train=100000,\n",
        "                      start_end_tokens=False,\n",
        "                      lower=True):\n",
        "\n",
        "    tokenizer = Tokenizer(lower=lower)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "\n",
        "    word_idx = tokenizer.word_index\n",
        "    idx_word = tokenizer.index_word\n",
        "    num_words = len(word_idx) + 1\n",
        "    word_counts = tokenizer.word_counts\n",
        "\n",
        "    print(f'There are {num_words} unique words.')\n",
        "\n",
        "    # import pickle\n",
        "    # with open('tokenizer.pickle', 'wb') as handle:\n",
        "    #     pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "    # Start-End tokens\n",
        "    # x = word_idx[\"start_token\"]\n",
        "    # y = word_idx[\"end_token\"]\n",
        "\n",
        "    # Limit to sequences with more than training length tokens\n",
        "    seq_lengths = [len(x) for x in sequences]\n",
        "    over_idx = [\n",
        "        i for i, l in enumerate(seq_lengths) if l > (training_length + result_length + 3)]\n",
        "\n",
        "    new_texts = []\n",
        "    new_sequences = []\n",
        "\n",
        "    # Only keep sequences with more than training length tokens\n",
        "    for i in over_idx:\n",
        "        new_texts.append(texts[i])\n",
        "        new_sequences.append(sequences[i])\n",
        "\n",
        "    training_seq = []\n",
        "    labels = []\n",
        "    training_seq_words = []\n",
        "    labels_words = []\n",
        "\n",
        "    for seq in new_sequences:\n",
        "\n",
        "        if len(training_seq) < max_train:\n",
        "            for i in range(training_length, len(seq) - result_length):\n",
        "                # Extract the features and label\n",
        "                extract = seq[i - training_length:i + result_length]\n",
        "                training_seq.append(extract[:training_length])\n",
        "                if start_end_tokens:\n",
        "                    label_adj = [x] + extract[training_length:] + [y]\n",
        "                else: label_adj = extract[training_length:]\n",
        "                labels.append(label_adj)\n",
        "\n",
        "                training_seq_words.append([idx_word[j] for j in extract[:training_length]])\n",
        "                labels_words.append([idx_word[j] for j in extract[training_length:]])\n",
        "\n",
        "    print(f'There are {len(training_seq)} training sequences.')\n",
        "\n",
        "    return word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, training_seq, labels, \\\n",
        "           training_seq_words, labels_words"
      ],
      "metadata": {
        "id": "8VcbDGZaSxmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len_text = 30\n",
        "len_result = 15"
      ],
      "metadata": {
        "id": "E1RFxZwPS2Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_idx, idx_word, num_words, word_counts, new_texts, sequences, features, labels, training_seq_words, labels_words = \\\n",
        "    sequence_generator(\n",
        "    data['text_clean'].tolist(), training_length = len_text, result_length = len_result, lower=True)"
      ],
      "metadata": {
        "id": "OT871qsaS0Ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_FRACTION = 0.7\n",
        "RANDOM_STATE = 50"
      ],
      "metadata": {
        "id": "cwoZ0V29S5Xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "jZ-nS8CJTE5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_train_valid(features,\n",
        "                       labels,\n",
        "                       num_words,\n",
        "                       train_fraction=TRAIN_FRACTION):\n",
        "\n",
        "    features, labels = shuffle(features, labels, random_state=RANDOM_STATE)\n",
        "    train_end = int(train_fraction * len(labels))\n",
        "\n",
        "    train_features = np.array(features[:train_end])\n",
        "    valid_features = np.array(features[train_end:])\n",
        "\n",
        "    train_labels = labels[:train_end]\n",
        "    valid_labels = labels[train_end:]\n",
        "\n",
        "    # Convert to arrays\n",
        "    X_train, X_valid = np.array(train_features), np.array(valid_features)\n",
        "\n",
        "    y_train = np.zeros((len(train_labels), num_words), dtype=np.int8)\n",
        "    y_valid = np.zeros((len(valid_labels), num_words), dtype=np.int8)\n",
        "\n",
        "    # numpy array with one-hot encoding consisting of number of training data \n",
        "    # and size of vocabulary with 1 at the corresponding word following from the features\n",
        "    for example_index, word_index in enumerate(train_labels):\n",
        "        y_train[example_index, word_index] = 1\n",
        "\n",
        "    for example_index, word_index in enumerate(valid_labels):\n",
        "        y_valid[example_index, word_index] = 1\n",
        "\n",
        "    return X_train, X_valid, y_train, y_valid"
      ],
      "metadata": {
        "id": "ynPJkUtpTGRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_tr, x_val, y_tr, y_val = create_train_valid(features, labels, num_words)"
      ],
      "metadata": {
        "id": "kS-7YyPeTIBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove = np.loadtxt('glove.6B.100d.txt', dtype='str', comments=None, encoding=\"utf8\")\n",
        "print(glove.shape)\n",
        "vectors = glove[:, 1:].astype('float')\n",
        "words = glove[:, 0]\n",
        "del glove\n",
        "\n",
        "word_lookup = {word: vector for word, vector in zip(words, vectors)}\n",
        "embedding_matrix = np.zeros((num_words, vectors.shape[1]))\n",
        "not_found = 0\n",
        "\n",
        "for i, word in enumerate(word_idx.keys()):\n",
        "    # Look up the word embedding\n",
        "    vector = word_lookup.get(word, None)\n",
        "\n",
        "    # Record in matrix\n",
        "    if vector is not None:\n",
        "        embedding_matrix[i + 1, :] = vector\n",
        "    else:\n",
        "        not_found += 1\n",
        "\n",
        "print(f'There were {not_found} words without pre-trained embeddings.')\n",
        "\n",
        "# Normalize and convert nan to 0\n",
        "embedding_matrix = embedding_matrix / \\\n",
        "    np.linalg.norm(embedding_matrix, axis=1).reshape((-1, 1))\n",
        "embedding_matrix = np.nan_to_num(embedding_matrix)"
      ],
      "metadata": {
        "id": "xj6WyYYBTJ_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "import gensim\n",
        "from numpy import *\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import warnings"
      ],
      "metadata": {
        "id": "7Uu0QZA2TVC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "latent_dim = 200\n",
        "# embedding_dim = 200"
      ],
      "metadata": {
        "id": "3cgwrq2dTXxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_inputs = Input(shape=(len_text,))"
      ],
      "metadata": {
        "id": "Uf0HeYbcTZW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_emb = Embedding(num_words, embedding_matrix.shape[1], embeddings_initializer=Constant(embedding_matrix), trainable=False)(encoder_inputs)"
      ],
      "metadata": {
        "id": "XyUKWJCBTaVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_lstm1 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)"
      ],
      "metadata": {
        "id": "MkGGSpwITbxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_lstm2 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "encoder_lstm3 = LSTM(latent_dim, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm3(encoder_output2)"
      ],
      "metadata": {
        "id": "DlKzy0RTTdkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_inputs = Input(shape=(None,))"
      ],
      "metadata": {
        "id": "5jEUWBPvTe-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dec_emb_layer = Embedding(num_words, embedding_matrix.shape[1], embeddings_initializer=Constant(embedding_matrix), trainable=False)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)"
      ],
      "metadata": {
        "id": "XLuAUcZ7Tf-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
        "decoder_outputs, decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "decoder_dense = TimeDistributed(Dense(num_words, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "metadata": {
        "id": "cQwUqA3LThqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "MVKgN5DhTj2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "duX0ARFTTls6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
        "\n",
        "history = model.fit([x_tr, y_tr[:, :-1]], y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)[:, 1:], epochs=50,\n",
        "                    callbacks=[es], batch_size=128,\n",
        "                    validation_data=([x_val, y_val[:, :-1]], y_val.reshape(y_val.shape[0], y_val.shape[1], 1)[:, 1:]))"
      ],
      "metadata": {
        "id": "dY-tm5JTTmfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot\n",
        "\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "XmJKymPkTog_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"fomc\")\n",
        "print(\"Saved model to disk\")"
      ],
      "metadata": {
        "id": "MYCUEh7cTqMp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}