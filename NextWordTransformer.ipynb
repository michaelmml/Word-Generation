{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NextWordTransformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNDW1RiDXIhV55DKzbnFiyD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RRJNM3x06WfZ"
      },
      "outputs": [],
      "source": [
        "#import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import logging\n",
        "# import tensorflow_text as text\n",
        "# from transformers import BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "ntD4LhJVUFkD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "xGjWyzHh6gGG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5dfc67c-9022-4d82-e7ce-7fc2bbcdac21"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pathlib\n",
        "root_folder='/content/drive/My Drive/WordGeneration'"
      ],
      "metadata": {
        "id": "2jwiYApr6igq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "import os\n",
        "import gc"
      ],
      "metadata": {
        "id": "OKM29_EHaDFT"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(root_folder+'/FOMC2021.txt', sep=\"\\n\")\n",
        "df.rename(columns={\"Action to Adopt Changes to the Committee's Rules Regarding Availability of Information\": \"text\"},\n",
        "          inplace=True)\n",
        "df[\"text\"] = df[\"text\"].str.replace(\"United States\", \"US\")\n",
        "df[\"text\"] = df[\"text\"].str.replace(\"U.S.\", \"US\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FTDcp5aJdOD",
        "outputId": "93078e40-c4db-43c0-e998-a5bed4f40e36"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 486 entries, 0 to 485\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    486 non-null    object\n",
            "dtypes: object(1)\n",
            "memory usage: 3.9+ KB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len_text = 40\n",
        "len_result = 1\n",
        "adj_lines = []\n",
        "max_len = 3"
      ],
      "metadata": {
        "id": "r0GRjyDWJfrC"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "JCyD21BFU07-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_cleaner(text):\n",
        "    newString = text.lower()\n",
        "    newString = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", newString)  # remove text in brackets\n",
        "    newString = re.sub(r\"'s\\b\", \"\", newString)\n",
        "    newString = re.sub(r'[^\\w\\d\\s\\-]+', '', newString)\n",
        "    newString = re.sub(r'[^\\w\\d\\s]+', ' ', newString)\n",
        "    newString = re.sub(r'\"', '', newString)  # removes quotation marks\n",
        "    newString = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", newString)  # all numbers\n",
        "    # newString = re.sub(r'(?<=[^\\s0-9])(?=[.,;:?])', r' ', newString)  # add space around punctuation\n",
        "    newString = re.sub(r'\\s\\s', ' ', newString)  # removes double spaces\n",
        "    return newString"
      ],
      "metadata": {
        "id": "21F_e2LNJgH-"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines = []\n",
        "for t in df[\"text\"]:\n",
        "    lines.append(text_cleaner(t))\n",
        "\n",
        "for i in range(len(lines)):\n",
        "    if len(lines[i].split()) >= max_len:\n",
        "        adj_lines.append(lines[i])"
      ],
      "metadata": {
        "id": "T5mhHZL8JjGt"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_sequences(texts,\n",
        "                   training_length=len_text,\n",
        "                   lower=True,\n",
        "                   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
        "\n",
        "    # Create the tokenizer object and train on texts\n",
        "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "\n",
        "    # Create look-up dictionaries and reverse look-ups\n",
        "    word_idx = tokenizer.word_index\n",
        "    idx_word = tokenizer.index_word\n",
        "    num_words = len(word_idx) + 1\n",
        "    word_counts = tokenizer.word_counts\n",
        "\n",
        "    print(f'There are {num_words} unique words.')\n",
        "\n",
        "    import pickle\n",
        "    # Save the tokenizer\n",
        "    with open('tokenizer.pickle', 'wb') as handle:\n",
        "         pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "    seq_lengths = [len(x) for x in sequences]\n",
        "    over_idx = [\n",
        "        i for i, l in enumerate(seq_lengths) if l > (training_length + len_result + 3)  # index and length of sequences\n",
        "    ]\n",
        "\n",
        "    new_texts = []\n",
        "    new_sequences = []\n",
        "\n",
        "    # Only keep sequences with more than training length tokens\n",
        "    for i in over_idx:\n",
        "        new_texts.append(texts[i])\n",
        "        new_sequences.append(sequences[i])\n",
        "\n",
        "    training_seq = []\n",
        "    labels = []\n",
        "\n",
        "    for seq in new_sequences:\n",
        "\n",
        "        if len(training_seq) < 50000:\n",
        "        # Create multiple training examples from each sequence\n",
        "            for i in range(training_length, len(seq) - len_result):\n",
        "                extract = seq[i - training_length:i + len_result]\n",
        "\n",
        "                training_seq.append(extract[:len_text])\n",
        "                labels.append(extract[len_text:])\n",
        "\n",
        "    print(f'There are {len(training_seq)} training sequences.')\n",
        "\n",
        "    return word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, training_seq, labels"
      ],
      "metadata": {
        "id": "bgc_-ffGJmtd"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_LENGTH = len_text\n",
        "filters = '!\"#$%&()*+/:<=>@[\\\\]^_`{|}~\\t\\n'\n",
        "word_idx, idx_word, num_words, word_counts, new_texts, sequences, features, labels = \\\n",
        "    make_sequences(\n",
        "    adj_lines, TRAINING_LENGTH, lower=True, filters=filters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cxM7hDsKIlO",
        "outputId": "1373a681-b529-41cd-cb2a-edd257a85b67"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2866 unique words.\n",
            "There are 38883 training sequences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def label_encode(labels, num_words):\n",
        "\n",
        "    y_encoded = np.zeros((len(labels), num_words), dtype=np.int8)\n",
        "\n",
        "    # One hot encoding of labels\n",
        "    for example_index, word_index in enumerate(labels):\n",
        "        y_encoded[example_index, word_index] = 1\n",
        "\n",
        "    return y_encoded"
      ],
      "metadata": {
        "id": "TfKmiqX_MNY_"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_labels = label_encode(labels, num_words)"
      ],
      "metadata": {
        "id": "-SElWWtQSj7h"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dataset \n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (features, encoded_labels))\n",
        "dataset = dataset.shuffle(len(np.array(df['text'])), reshuffle_each_iteration=True).batch(\n",
        "    BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "Rpei7YsSSB5S"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features, labels = next(iter(dataset))"
      ],
      "metadata": {
        "id": "IK8C_n94ebjN"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkH1De1-e4AI",
        "outputId": "708d83b9-972a-4d2c-b767-5ad9b3ab1bf8"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 40), dtype=int32, numpy=\n",
              "array([[  91,   38,   92, ...,   93,  110,   31],\n",
              "       [   5,    1,  329, ...,  128,   26,   17],\n",
              "       [   8,   57,   80, ...,  123,    1,  236],\n",
              "       ...,\n",
              "       [ 317,   23,   54, ...,   43,  158,  265],\n",
              "       [  76,    8,    1, ...,    1,  137,   74],\n",
              "       [1024,  924,    2, ...,  264,    4,   25]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXnp6txLe6gE",
        "outputId": "2cdd8289-de95-4d0f-e648-4e16c8d1d9fe"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 2866), dtype=int8, numpy=\n",
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 1, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 1, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 1, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]], dtype=int8)>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "print('Start building tokenizer ...')\n",
        "tokenizer_en = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "print('End building tokenizer ...')"
      ],
      "metadata": {
        "id": "jMWSa2-a_Ra-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Summarisationv2')"
      ],
      "metadata": {
        "id": "oTim_jo4VQ6e"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import TransformerArchitecture\n",
        "from TransformerArchitecture import *"
      ],
      "metadata": {
        "id": "AH4fxnndVjn9"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters for the model\n",
        "D_MODEL = 512 # 512\n",
        "N_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "N_HEADS = 8 # 8\n",
        "DROPOUT_RATE = 0.1 # 0.1"
      ],
      "metadata": {
        "id": "PEnmeuXMYy1r"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderTransformer(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 d_model,\n",
        "                 n_layers,\n",
        "                 FFN_units,\n",
        "                 n_heads,\n",
        "                 dropout_rate,\n",
        "                 name=\"EncoderTransformer\"):\n",
        "        super(EncoderTransformer, self).__init__(name=name)\n",
        "        \n",
        "        # Build the encoder\n",
        "        self.encoder = Encoder(n_layers,\n",
        "                               FFN_units,\n",
        "                               n_heads,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "\n",
        "        # build the linear transformation and softmax function\n",
        "        self.last_linear = layers.Dense(units=vocab_size_enc, name=\"lin_output\")\n",
        "    \n",
        "    def call(self, enc_inputs, training):\n",
        "        # Create the padding mask for the encoder\n",
        "        enc_mask = create_padding_mask(enc_inputs)\n",
        "        # Call the encoder\n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        # Call the Linear and Softmax functions\n",
        "        outputs = self.last_linear(enc_outputs)\n",
        "        \n",
        "        return outputs"
      ],
      "metadata": {
        "id": "_EKotWVLVlCW"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(target, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    loss_ = loss_object(target, pred)\n",
        "    \n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "        \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "metadata": {
        "id": "cgTVOjY6Y80l"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_train(dataset, transformer, n_epochs, print_every=50):\n",
        "  ''' Train the transformer model for n_epochs using the data generator dataset'''\n",
        "  losses = []\n",
        "  accuracies = []\n",
        "  # In every epoch\n",
        "  for epoch in range(n_epochs):\n",
        "    print(\"Starting epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    # Get a batch of inputs and targets\n",
        "    for (batch, (enc_inputs, labels)) in enumerate(dataset):\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Call the transformer and get the predicted output\n",
        "            predictions = transformer(enc_inputs, True)\n",
        "            # Calculate the loss\n",
        "            loss = loss_function(labels, predictions)\n",
        "        # Update the weights and optimizer\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "        # Save and store the metrics\n",
        "        train_loss(loss)\n",
        "        train_accuracy(labels, predictions)\n",
        "        \n",
        "        if batch % print_every == 0:\n",
        "            losses.append(train_loss.result())\n",
        "            accuracies.append(train_accuracy.result())\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
        "            \n",
        "    # Checkpoint the model on every epoch        \n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Saving checkpoint for epoch {} in {}\".format(epoch+1,\n",
        "                                                        ckpt_save_path))\n",
        "    print(\"Time for 1 epoch: {} secs\\n\".format(time.time() - start))\n",
        "\n",
        "  return losses, accuracies"
      ],
      "metadata": {
        "id": "6lxaeAroZBma"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the session\n",
        "tf.keras.backend.clear_session()\n",
        "# Create the Transformer model\n",
        "transformer = EncoderTransformer(vocab_size_enc=num_words,\n",
        "                          d_model=D_MODEL,\n",
        "                          n_layers=N_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          n_heads=N_HEADS,\n",
        "                          dropout_rate=DROPOUT_RATE)\n",
        "\n",
        "# Define a categorical cross entropy loss\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction=\"none\")\n",
        "# Define a metric to store the mean loss of every epoch\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "# Define a matric to save the accuracy in every epoch\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\n",
        "# Create the scheduler for learning rate decay\n",
        "leaning_rate = CustomSchedule(D_MODEL)\n",
        "# Create the Adam optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "metadata": {
        "id": "KPU2bsAgZTJK"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the Checkpoint \n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, root_folder, max_to_keep=2)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Last checkpoint restored.\")"
      ],
      "metadata": {
        "id": "hoySVYo4ZliY"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "losses, accuracies = main_train(dataset, transformer, 5, 100)"
      ],
      "metadata": {
        "id": "9ibJQLACaMEf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}